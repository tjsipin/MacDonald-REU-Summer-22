---
title: "RF_monthly_short"
author: "TJ Sipin"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(randomForest)
library(ggplot2)
library(finalfit)
library(dplyr)
library(naniar)
library(readr)
library(tidyverse)
library(caret)
library(readr)
library(dplyr)
library(mlbench)
library(rsample)
library(tidyverse)
library(tidymodels)
library(recipes)
```

## Libraries and Data

We try our random forest on the monthly data, taking only years 2014 to 2017. 2018 can work but there are some missing values for cutaneous leishmaniasis.

### Description

Plan to take early prediction on binary classification to feed into model to produce # of CL cases as output. Improved MAPE by about 10% compared to previous regression model!

```{r packages}
diff_data <- read_csv('../data/diff_data.csv')

data <- diff_data %>% 
  select(-c(...1)) %>% 
  filter(Year %in% c(2014, 2015, 2016, 2017)) %>% 
  filter(CL > 0) 

quantile_data <- data %>% 
  filter(CL > 0) %>% 
  select(CL)

quantile <- (quantile_data$CL %>% 
  quantile(.4))[[1]]

data_cat <- data %>% 
  mutate(CL_cat = case_when(CL < quantile ~ 'low',
                         CL >= quantile ~ 'high') %>% 
           as_factor()) %>% 
  select(-c(Name, Year, Code, Country, TEMP_pland_forest, TEMP_te_forest, TEMP_enn_mn_forest, TEMP_area_mn_forest))

# data partitioning

set.seed(123)

split_cat <- initial_split(data_cat, prop = 0.7, strata = CL_cat)
train_cat <- training(split_cat) %>% as.data.frame()
test_cat <- testing(split_cat)




rf_recipe <-
  recipe(
    CL ~ .,
    data = train_cat) %>% 
  prep()

rf_testing <- rf_recipe %>% 
  bake(test_cat)

rf_training <- juice(rf_recipe)

rf_model <- rand_forest(mtry = 7,
                        trees = 700) %>% 
  set_engine('ranger', importance = 'permutation',
             seed = 123, quantreg = T) %>%
  set_mode('regression') 


preds_bind <- function(data_fit, lower = 0.1, upper = 0.9) {
  predict(
    rf_wf$fit$fit$fit,
    extract_recipe(rf_wf) %>% 
      bake(data_fit),
    type = 'quantiles',
    quantiles = c(lower, upper, 0.50)
  ) %>% 
    with(predictions) %>% 
    as_tibble() %>%
    set_names(paste0('.pred', c('_lower', '_upper', ''))) %>% 
    # mutate(across(contains('.pred'), ~10^.x)) %>%
    bind_cols(data_fit) %>% 
    select(contains('.pred'), CL, Month, CL_cat, Population,
           LST_Day, LST_Night, NDVI, EVI, Precip, AvgRad, SWOccurrence,
           Muni_TotalArea, TEMP_diff_pland_forest, TEMP_diff_area_mn_forest,
           TEMP_diff_enn_mn_forest, TEMP_diff_te_forest,
           pland_forest, area_mn_forest, enn_mn_forest, te_forest)
}

rf_wf <- workflow() %>%
  add_model(rf_model) %>%
  add_recipe(rf_recipe) %>%
  fit(train_cat)

rf_preds_test <- preds_bind(test_cat)

set.seed(123)
rf_preds_test %>% 
  mutate(pred_interval = cut_number(CL, n = 0.01)) %>% 
  group_by(pred_interval) %>% 
  sample_n(1000) %>% 
  ggplot(aes(x = .pred)) +
  geom_point(aes(y = .pred, color = 'prediction interval')) +
  geom_errorbar(aes(ymin = .pred_lower, ymax = .pred_upper, color = 'prediction interval')) + 
  geom_point(aes(y = CL, color = 'actuals'),
             alpha = 0.7) + 
  scale_x_log10() +
  scale_y_log10()

rf_preds_train <- preds_bind(train_cat)

bind_rows(
  mape(rf_preds_train, CL, .pred),
  mape(rf_preds_test, CL, .pred)) %>% 
  mutate(dataset = c('training', 'holdout')) %>% 
  gt::gt() %>% 
  gt::fmt_number('.estimate', decimals = 1)
```


# Sanity Check

## To see where to split

```{r}
## 30
quantile_data <- data %>% 
  filter(CL_fac > 0) %>% 
  select(CL_fac)

quantile <- (quantile_data$CL_fac %>% 
  quantile(.3))[[1]]

data_cat <- data %>% 
  mutate(CL_cont = case_when(CL_fac < quantile ~ 'low',
                         CL_fac >= quantile ~ 'high') %>% 
           as_factor()) %>% 
  select(-c(Name, Year, Code, Country, TEMP_pland_forest, TEMP_te_forest, TEMP_enn_mn_forest, TEMP_area_mn_forest))

set.seed(123)

# data_sample <- sample_n(data, size = nrow(data) * 0.7)

split_cat <- initial_split(data_cat, prop = 0.7, strata = CL_cont)
train_cat <- training(split_cat) %>% as.data.frame()
test_cat <- testing(split_cat)


library(tidyverse)
library(tidymodels)

rf_recipe <-
  recipe(
    CL_cont ~ .,
    data = train_cat) %>% 
  step_rm(CL_fac) %>% 
  prep()

rf_testing <- rf_recipe %>% 
  bake(test_cat)

rf_training <- juice(rf_recipe)

rf_model <- rand_forest(mtry = 7,
                        trees = 700) %>% 
  set_engine('ranger', importance = 'impurity',
             seed = 123) %>%
  set_mode('classification') %>% 
  fit(CL_cont ~ ., data = train_cat %>% select(-c(CL_fac))) 

set.seed(123)

# rf_wf <- workflow() %>% 
#   add_model(rf_model) %>% 
#   add_recipe(rf_recipe) %>% 
#   fit(train_cat)

testing_data_later_30 <- rf_model %>% 
  predict(rf_testing) %>% 
  bind_cols(rf_testing) 

confusionMatrix(data = testing_data_later_30$.pred_class, reference = testing_data_later_30$CL_cont)

## 40

quantile_data <- data %>% 
  filter(CL_fac > 0) %>% 
  select(CL_fac)

quantile <- (quantile_data$CL_fac %>% 
  quantile(.4))[[1]]

data_cat <- data %>% 
  mutate(CL_cont = case_when(CL_fac < quantile ~ 'low',
                         CL_fac >= quantile ~ 'high') %>% 
           as_factor()) %>% 
  select(-c(Name, Year, Code, Country, TEMP_pland_forest, TEMP_te_forest, TEMP_enn_mn_forest, TEMP_area_mn_forest))

set.seed(123)

# data_sample <- sample_n(data, size = nrow(data) * 0.7)

split_cat <- initial_split(data_cat, prop = 0.7, strata = CL_cont)
train_cat <- training(split_cat) %>% as.data.frame()
test_cat <- testing(split_cat)




library(tidyverse)
library(tidymodels)

rf_recipe <-
  recipe(
    CL_cont ~ .,
    data = train_cat) %>% 
  prep()

rf_testing <- rf_recipe %>% 
  bake(test_cat)

rf_training <- juice(rf_recipe)

rf_model <- rand_forest(mtry = 7,
                        trees = 700) %>% 
  set_engine('ranger', importance = 'impurity',
             seed = 123) %>%
  set_mode('classification') %>% 
  fit(CL_cont ~ ., data = train_cat %>% select(-c(CL_fac))) 

set.seed(123)

# rf_wf <- workflow() %>% 
#   add_model(rf_model) %>% 
#   add_recipe(rf_recipe) %>% 
#   fit(train_cat)

testing_data_later_40 <- rf_model %>% 
  predict(rf_testing) %>% 
  bind_cols(rf_testing) 

confusionMatrix(data = testing_data_later_40$.pred_class, reference = testing_data_later_40$CL_cont)

## 50

quantile_data <- data %>% 
  filter(CL_fac > 0) %>% 
  select(CL_fac)

quantile <- (quantile_data$CL_fac %>% 
  quantile(.5))[[1]]

data_cat <- data %>% 
  mutate(CL_cont = case_when(CL_fac < quantile ~ 'low',
                         CL_fac >= quantile ~ 'high') %>% 
           as_factor()) %>% 
  select(-c(Name, Year, Code, Country, TEMP_pland_forest, TEMP_te_forest, TEMP_enn_mn_forest, TEMP_area_mn_forest))

set.seed(123)

# data_sample <- sample_n(data, size = nrow(data) * 0.7)

split_cat <- initial_split(data_cat, prop = 0.7, strata = CL_cont)
train_cat <- training(split_cat) %>% as.data.frame()
test_cat <- testing(split_cat)




library(tidyverse)
library(tidymodels)

rf_recipe <-
  recipe(
    CL_cont ~ .,
    data = train_cat) %>% 
  prep()

rf_testing <- rf_recipe %>% 
  bake(test_cat)

rf_training <- juice(rf_recipe)

rf_model <- rand_forest(mtry = 7,
                        trees = 700) %>% 
  set_engine('ranger', importance = 'impurity',
             seed = 123) %>%
  set_mode('classification') %>% 
  fit(CL_cont ~ ., data = train_cat %>% select(-c(CL_fac))) 

set.seed(123)

# rf_wf <- workflow() %>% 
#   add_model(rf_model) %>% 
#   add_recipe(rf_recipe) %>% 
#   fit(train_cat)

testing_data_later_50 <- rf_model %>% 
  predict(rf_testing) %>% 
  bind_cols(rf_testing) 

confusionMatrix(data = testing_data_later_50$.pred_class, reference = testing_data_later_50$CL_cont)

## 60

quantile_data <- data %>% 
  filter(CL_fac > 0) %>% 
  select(CL_fac)

quantile <- (quantile_data$CL_fac %>% 
  quantile(.6))[[1]]

data_cat <- data %>% 
  mutate(CL_cont = case_when(CL_fac < quantile ~ 'low',
                         CL_fac >= quantile ~ 'high') %>% 
           as_factor()) %>% 
  select(-c(Name, Year, Code, Country, TEMP_pland_forest, TEMP_te_forest, TEMP_enn_mn_forest, TEMP_area_mn_forest))

set.seed(123)

# data_sample <- sample_n(data, size = nrow(data) * 0.7)

split_cat <- initial_split(data_cat, prop = 0.7, strata = CL_cont)
train_cat <- training(split_cat) %>% as.data.frame()
test_cat <- testing(split_cat)




library(tidyverse)
library(tidymodels)

rf_recipe <-
  recipe(
    CL_cont ~ .,
    data = train_cat) %>% 
  prep()

rf_testing <- rf_recipe %>% 
  bake(test_cat)

rf_training <- juice(rf_recipe)

rf_model <- rand_forest(mtry = 7,
                        trees = 700) %>% 
  set_engine('ranger', importance = 'impurity',
             seed = 123) %>%
  set_mode('classification') %>% 
  fit(CL_cont ~ ., data = train_cat %>% select(-c(CL_fac))) 

set.seed(123)

# rf_wf <- workflow() %>% 
#   add_model(rf_model) %>% 
#   add_recipe(rf_recipe) %>% 
#   fit(train_cat)

testing_data_later_60 <- rf_model %>% 
  predict(rf_testing) %>% 
  bind_cols(rf_testing) 

confusionMatrix(data = testing_data_later_60$.pred_class, reference = testing_data_later_60$CL_cont)

## 70
quantile_data <- data %>% 
  filter(CL_fac > 0) %>% 
  select(CL_fac)

quantile <- (quantile_data$CL_fac %>% 
  quantile(.7))[[1]]

data_cat <- data %>% 
  mutate(CL_cont = case_when(CL_fac < quantile ~ 'low',
                         CL_fac >= quantile ~ 'high') %>% 
           as_factor()) %>% 
  select(-c(Name, Year, Code, Country, TEMP_pland_forest, TEMP_te_forest, TEMP_enn_mn_forest, TEMP_area_mn_forest))

set.seed(123)

# data_sample <- sample_n(data, size = nrow(data) * 0.7)

split_cat <- initial_split(data_cat, prop = 0.7, strata = CL_cont)
train_cat <- training(split_cat) %>% as.data.frame()
test_cat <- testing(split_cat)




library(tidyverse)
library(tidymodels)

rf_recipe <-
  recipe(
    CL_cont ~ .,
    data = train_cat) %>% 
  prep()

rf_testing <- rf_recipe %>% 
  bake(test_cat)

rf_training <- juice(rf_recipe)

rf_model <- rand_forest(mtry = 7,
                        trees = 700) %>% 
  set_engine('ranger', importance = 'impurity',
             seed = 123) %>%
  set_mode('classification') %>% 
  fit(CL_cont ~ ., data = train_cat %>% select(-c(CL_fac))) 

set.seed(123)

# rf_wf <- workflow() %>% 
#   add_model(rf_model) %>% 
#   add_recipe(rf_recipe) %>% 
#   fit(train_cat)

testing_data_later_70 <- rf_model %>% 
  predict(rf_testing) %>% 
  bind_cols(rf_testing) 

confusionMatrix(data = testing_data_later_70$.pred_class, reference = testing_data_later_70$CL_cont)


save(testing_data_later_30, file = 'data/CL_RF_testing_data_later_30')
save(testing_data_later_40, file = 'data/CL_RF_testing_data_later_40')
save(testing_data_later_50, file = 'data/CL_RF_testing_data_later_50')
save(testing_data_later_60, file = 'data/CL_RF_testing_data_later_60')
save(testing_data_later_70, file = 'data/CL_RF_testing_data_later_70')
```

```{r}
library(tidyverse)
library(pROC)
library(dplyr)
library(ggplot2)
library(plotROC)

load(file = 'data/CL_RF_testing_data_later_30')
load(file = 'data/CL_RF_testing_data_later_40')
load(file = 'data/CL_RF_testing_data_later_50')
load(file = 'data/CL_RF_testing_data_later_60')
load(file = 'data/CL_RF_testing_data_later_70')

roc_30 <- roc(testing_data_later_30$CL_cont,
              testing_data_later_30$.pred_class %>% as.numeric(),
              print.auc = T)

roc_40 <- roc(testing_data_later_40$CL_cont,
              testing_data_later_40$.pred_class %>% as.numeric(),
              print.auc = T)

roc_50 <- roc(testing_data_later_50$CL_cont,
              testing_data_later_50$.pred_class %>% as.numeric(),
              print.auc = T)

roc_60 <- roc(testing_data_later_60$CL_cont,
              testing_data_later_60$.pred_class %>% as.numeric(),
              print.auc = T)

roc_70 <- roc(testing_data_later_70$CL_cont,
              testing_data_later_70$.pred_class %>% as.numeric(),
              print.auc = T)

roc_list <- list('30th percentile' = roc_30,
           '40th percentile' = roc_40,
           '50th percentile' = roc_50,
           '60th percentile' = roc_60,
           '70th percentile' = roc_70)

# extract AUC
data.auc <- roc_list %>% 
  map(~tibble(AUC = .x$auc)) %>% 
  bind_rows(.id = 'name')

# generate labels
data.labels <- data.auc %>% 
  mutate(label_long = paste0(name, ' , AUC = ',
                             paste(round(AUC,2))),
         label_AUC = paste0('AUC = ',
                            paste(round(AUC,2))))

pROC::ggroc(roc_list,
           legacy.axes = F) +
  scale_color_discrete(labels = data.labels$label_long)
```

Confirmed that 40th percentile is a good option.

# DALEX

```{r}
library(DALEX)
library(plotly)
library(gridExtra)
library(archivist)
```

```{r}
library(randomForest)
library(ggplot2)
library(finalfit)
library(dplyr)
library(naniar)
library(readr)
library(tidyverse)
library(caret)
library(readr)
library(dplyr)
library(mlbench)
library(rsample)
library(tidyverse)
library(tidymodels)

diff_data <- read_csv('../data/diff_data.csv')

data <- diff_data %>% 
  select(-c(...1)) %>% 
  filter(Year %in% c(2014, 2015, 2016, 2017)) %>% 
  filter(CL > 0) 

quantile_data <- data %>% 
  filter(CL > 0) %>% 
  select(CL)

quantile <- (quantile_data$CL %>% 
  quantile(.4))[[1]]

data_cat <- data %>% 
  mutate(CL_cat = case_when(CL < quantile ~ 'low',
                         CL >= quantile ~ 'high') %>% 
           as_factor()) %>% 
  select(-c(Name, Year, Code, Country, TEMP_pland_forest, TEMP_te_forest, TEMP_enn_mn_forest, TEMP_area_mn_forest))

# data partitioning

set.seed(123)

split_cat <- initial_split(data_cat, prop = 0.7, strata = CL_cat)
train_cat <- training(split_cat) %>% as.data.frame()
test_cat <- testing(split_cat)


## 40



library(tidyverse)
library(tidymodels)

rf_recipe <-
  recipe(
    CL ~ .,
    data = train_cat) %>% 
  prep()

rf_testing <- rf_recipe %>% 
  bake(test_cat)

rf_training <- juice(rf_recipe)

rf_model <- rand_forest(mtry = 7,
                        trees = 700) %>% 
  set_engine('ranger', importance = 'impurity',
             seed = 123) %>%
  set_mode('classification') %>% 
  fit(CL_cat ~ ., data = train_cat %>% select(-c(CL))) 

set.seed(123)

# rf_wf <- workflow() %>% 
#   add_model(rf_model) %>% 
#   add_recipe(rf_recipe) %>% 
#   fit(train_cat)

testing_data_later_40 <- rf_model %>% 
  predict(rf_testing) %>% 
  bind_cols(predict(rf_model, rf_testing, type = 'prob')) %>% 
  bind_cols(rf_testing) 

confusionMatrix(data = testing_data_later_40$.pred_class, reference = testing_data_later_40$CL_cat)


target_as_int <- as.integer(test_cat$CL_cat)
explainer_rf <- DALEX::explain(model = rf_model,
                        data = test_cat %>% 
                          select(-c(CL_cat)),
                        y = target_as_int,
                        label = 'Random Forest')

save(explainer_rf, file = 'explainer_rf_CL_monthly_diff')
```



```{r}

```




```{r}
dep_diff <- model_profile(explainer = explainer_rf,
                            variables = c('TEMP_diff_pland_forest',
                                          'TEMP_diff_te_forest',
                                          'TEMP_diff_enn_mn_forest',
                                          'TEMP_diff_area_mn_forest',
                                          'Muni_TotalArea',
                                          'te_forest',
                                          'enn_mn_forest',
                                          'area_mn_forest',
                                          'pland_forest',
                                          'SWOccurrence',
                                          'AvgRad',
                                          'Precip',
                                          'NDVI',
                                          'LST_Day',
                                          'Population',
                                          'Month'
                                          ),
                            type = 'conditional',
                            N = 10)
plot(dep_diff, geom = 'profiles') %>% ggplotly(height = 1600)
```
## Variable Importance
```{r}
variable_importance(explainer_rf) %>% plot()
```
## Local Interpretation

[uc-r.github.io/dalex]

```{r}
set.seed(123)

random_row <- sample(1:nrow(test_cat), size = 1) # 2463

# Create a single observation (randomized)
new_cust <- test_cat[random_row,] %>% as.data.frame()

# Compute breakdown distance
new_cust_rf <- predict_parts_break_down(explainer_rf, new_observation = new_cust)

# class of predict_parts_break_down output
class(new_cust_rf)

# see top 10 influential variables for the observation
new_cust_rf[1:10, 1:5]

# plot
new_cust_rf %>% plot()
```


TODO:

- create rf with new variable using .pred_low and .pred_high, perhaps create new column making .pred_low negative and keeping .pred_high positive. That is, a new variable with space [-1, 1] where -1 is sure to be .pred_low and 1 is sure to be .pred_high.
  - output 

```{r}
glm(CL ~ . - CL_cat - .pred_class - NDVI - LST_Day - Muni_TotalArea - 
     enn_mn_forest - TEMP_diff_pland_forest - TEMP_diff_enn_mn_forest -
     Population - AvgRad - TEMP_diff_te_forest - Precip -
     SWOccurrence - TEMP_diff_area_mn_forest - .pred_high, data = testing_data_later_40,
    family = 'quasibinomial') %>% 
  plot()
```

```{r}
testing_data_later_40
```


```{r}
prob_data <- testing_data_later_40 %>% 
  mutate(pred_prob = (2*.pred_high - 1)) %>%
  # mutate(pred_prob = .pred_high) %>%
  select(-c(CL_cat, .pred_low, .pred_high))

split <- initial_split(prob_data, prop = 0.7, strata = CL)
training <- training(split)
testing <- testing(split)

rf_recipe <-
  recipe(
    CL ~ .,
    data = training) %>% 
  prep()

rf_testing <- rf_recipe %>% 
  bake(testing)

rf_training <- juice(rf_recipe)

rf_model <- rand_forest(mtry = 7,
                        trees = 500) %>% 
  set_engine('ranger', importance = 'permutation',
             seed = 123, quantreg = TRUE) %>% 
  set_mode('regression')

rf_wf <- workflow() %>%
  add_model(rf_model) %>%
  add_recipe(rf_recipe) %>%
  fit(training) 

preds_bind <- function(data_fit, lower = 0.1, upper = 0.9) {
  predict(
    rf_wf$fit$fit$fit,
    extract_recipe(rf_wf) %>% 
      bake(data_fit),
    type = 'quantiles',
    quantiles = c(lower, upper, 0.50)
  ) %>% 
    with(predictions) %>% 
    as_tibble() %>%
    set_names(paste0('.pred', c('_lower', '_upper', ''))) %>% 
    # mutate(across(contains('.pred'), ~10^.x)) %>%
    bind_cols(data_fit) %>% 
    select(c(contains('.pred'), pred_prob, CL, Population,
           LST_Day, NDVI, EVI, Precip, AvgRad, SWOccurrence,
           pland_forest, area_mn_forest, enn_mn_forest, te_forest,
           .pred_class))
}

rf_preds_test <- preds_bind(testing)

set.seed(123)
rf_preds_test %>% 
  mutate(pred_interval = cut_number(CL, n = 0.01)) %>% 
  group_by(pred_interval) %>% 
  sample_n(1000) %>% 
  ggplot(aes(x = .pred)) +
  geom_point(aes(y = .pred, color = 'prediction interval')) +
  geom_errorbar(aes(ymin = .pred_lower, ymax = .pred_upper, color = 'prediction interval')) + 
  geom_point(aes(y = CL, color = 'actuals'),
             alpha = 0.7) + 
  scale_x_log10() +
  scale_y_log10()

rf_preds_train <- preds_bind(training)

bind_rows(
  mape(rf_preds_train, CL, .pred),
  mape(rf_preds_test, CL, .pred)) %>% 
  mutate(dataset = c('training', 'holdout')) %>% 
  gt::gt() %>% 
  gt::fmt_number('.estimate', decimals = 1)
```

[https://www.r-bloggers.com/2020/05/using-xgboost-with-tidymodels/]

```{r}
library(knitr)
```


```{r}
## 30
quantile_data <- data %>% 
  filter(CL > 0) %>% 
  select(CL)

quantile <- (quantile_data$CL %>% 
  quantile(.3))[[1]]

data_cat <- data %>%
  mutate(CL_cat = case_when(CL < quantile ~ 'low',
                         CL >= quantile ~ 'high') %>% 
           as_factor()) %>% 
  select(-c(TEMP_pland_forest, TEMP_te_forest, TEMP_enn_mn_forest, TEMP_area_mn_forest))

set.seed(123)

# data_sample <- sample_n(data, size = nrow(data) * 0.7)

split_cat <- initial_split(data_cat, prop = 0.7, strata = CL_cat)
train_cat <- training(split_cat) %>% as.data.frame()
test_cat <- testing(split_cat)


library(tidyverse)
library(tidymodels)

# preprocessing recipe
xgb_recipe <-
  recipe(
    CL_cat ~ .,
    data = train_cat) %>% 
  step_rm(CL, Name, Year, Code, Country) %>% 
  prep()

# splitting for cross validation
xgb_cv_folds <- xgb_recipe %>% 
  bake(train_cat) %>% 
  vfold_cv(v = 5)

# xgboost model specification

xgb_model <- boost_tree(
  mode = 'classification',
  trees = 1000,
  min_n = tune(),
  tree_depth = tune(),
  learn_rate = tune(),
  loss_reduction = tune()
  ) %>% 
  set_engine('xgboost', 
             objective ='binary:logistic')

set.seed(123)

# grid specification

xgb_params <- parameters(
  min_n(),
  tree_depth(),
  learn_rate(),
  loss_reduction()
)

xgb_grid <- grid_max_entropy(
  xgb_params,
  size = 60
)

# define the workflow

xgb_wf <- workflow() %>% 
  add_model(xgb_model) %>% 
  add_formula(CL_cat ~ .)

# tune the model
## hyperparameter tuning

xgb_tuned_30 <- tune_grid(
  object = xgb_wf,
  resamples = xgb_cv_folds,
  grid = xgb_grid,
  metrics = metric_set(sensitivity, accuracy),
  control = control_grid(verbose = T)
)

xgb_tuned_30 %>% 
  show_best(metric = 'accuracy') 

save(xgb_tuned_30, file = 'data/CL_xgb_tuned_30')

## 40
quantile_data <- data %>% 
  filter(CL > 0) %>% 
  select(CL)

quantile <- (quantile_data$CL %>% 
  quantile(.4))[[1]]

data_cat <- data %>% 
  mutate(CL_cat = case_when(CL < quantile ~ 'low',
                         CL >= quantile ~ 'high') %>% 
           as_factor()) %>% 
  select(-c(TEMP_pland_forest, TEMP_te_forest, TEMP_enn_mn_forest, TEMP_area_mn_forest))

set.seed(123)

# data_sample <- sample_n(data, size = nrow(data) * 0.7)

split_cat <- initial_split(data_cat, prop = 0.7, strata = CL_cat)
train_cat <- training(split_cat) %>% as.data.frame()
test_cat <- testing(split_cat)


library(tidyverse)
library(tidymodels)

# preprocessing recipe
xgb_recipe <-
  recipe(
    CL_cat ~ .,
    data = train_cat) %>% 
  step_rm(CL, Name, Year, Code, Country) %>% 
  prep()

# splitting for cross validation
xgb_cv_folds <- xgb_recipe %>% 
  bake(train_cat) %>% 
  vfold_cv(v = 5)

# xgboost model specification

xgb_model <- boost_tree(
  mode = 'classification',
  trees = 1000,
  min_n = tune(),
  tree_depth = tune(),
  learn_rate = tune(),
  loss_reduction = tune()
  ) %>% 
  set_engine('xgboost', 
             objective ='binary:logistic')

set.seed(123)

# grid specification

xgb_params <- parameters(
  min_n(),
  tree_depth(),
  learn_rate(),
  loss_reduction()
)

xgb_grid <- grid_max_entropy(
  xgb_params,
  size = 60
)


# define the workflow

xgb_wf <- workflow() %>% 
  add_model(xgb_model) %>% 
  add_formula(CL_cat ~ .)

# tune the model
## hyperparameter tuning

xgb_tuned_40 <- tune_grid(
  object = xgb_wf,
  resamples = xgb_cv_folds,
  grid = xgb_grid,
  metrics = metric_set(sensitivity, accuracy),
  control = control_grid(verbose = T)
)

save(xgb_tuned_40, file = 'data/CL_xgb_tuned_40')

## 50
quantile_data <- data %>% 
  filter(CL > 0) %>% 
  select(CL)

quantile <- (quantile_data$CL %>% 
  quantile(.5))[[1]]

data_cat <- data %>% 
  mutate(CL_cat = case_when(CL < quantile ~ 'low',
                         CL >= quantile ~ 'high') %>% 
           as_factor()) %>% 
  select(-c(TEMP_pland_forest, TEMP_te_forest, TEMP_enn_mn_forest, TEMP_area_mn_forest))

set.seed(123)

# data_sample <- sample_n(data, size = nrow(data) * 0.7)

split_cat <- initial_split(data_cat, prop = 0.7, strata = CL_cat)
train_cat <- training(split_cat) %>% as.data.frame()
test_cat <- testing(split_cat)


library(tidyverse)
library(tidymodels)

# preprocessing recipe
xgb_recipe <-
  recipe(
    CL_cat ~ .,
    data = train_cat) %>% 
  step_rm(CL, Name, Year, Code, Country) %>% 
  prep()

# splitting for cross validation
xgb_cv_folds <- xgb_recipe %>% 
  bake(train_cat) %>% 
  vfold_cv(v = 5)

# xgboost model specification

xgb_model <- boost_tree(
  mode = 'classification',
  trees = 1000,
  min_n = tune(),
  tree_depth = tune(),
  learn_rate = tune(),
  loss_reduction = tune()
  ) %>% 
  set_engine('xgboost', 
             objective ='binary:logistic')

set.seed(123)

# grid specification

xgb_params <- parameters(
  min_n(),
  tree_depth(),
  learn_rate(),
  loss_reduction()
)

xgb_grid <- grid_max_entropy(
  xgb_params,
  size = 60
)


# define the workflow

xgb_wf <- workflow() %>% 
  add_model(xgb_model) %>% 
  add_formula(CL_cat ~ .)

# tune the model
## hyperparameter tuning

xgb_tuned_50 <- tune_grid(
  object = xgb_wf,
  resamples = xgb_cv_folds,
  grid = xgb_grid,
  metrics = metric_set(sensitivity, accuracy),
  control = control_grid(verbose = T)
)

save(xgb_tuned_50, file = 'data/CL_xgb_tuned_50')

## 60
quantile_data <- data %>% 
  filter(CL > 0) %>% 
  select(CL)

quantile <- (quantile_data$CL %>% 
  quantile(.6))[[1]]

data_cat <- data %>% 
  mutate(CL_cat = case_when(CL < quantile ~ 'low',
                         CL >= quantile ~ 'high') %>% 
           as_factor()) %>% 
  select(-c(TEMP_pland_forest, TEMP_te_forest, TEMP_enn_mn_forest, TEMP_area_mn_forest))

set.seed(123)

# data_sample <- sample_n(data, size = nrow(data) * 0.7)

split_cat <- initial_split(data_cat, prop = 0.7, strata = CL_cat)
train_cat <- training(split_cat) %>% as.data.frame()
test_cat <- testing(split_cat)


library(tidyverse)
library(tidymodels)

# preprocessing recipe
xgb_recipe <-
  recipe(
    CL_cat ~ .,
    data = train_cat) %>% 
  step_rm(CL, Name, Year, Code, Country) %>% 
  prep()

# splitting for cross validation
xgb_cv_folds <- xgb_recipe %>% 
  bake(train_cat) %>% 
  vfold_cv(v = 5)

# xgboost model specification

xgb_model <- boost_tree(
  mode = 'classification',
  trees = 1000,
  min_n = tune(),
  tree_depth = tune(),
  learn_rate = tune(),
  loss_reduction = tune()
  ) %>% 
  set_engine('xgboost', 
             objective ='binary:logistic')

set.seed(123)

# grid specification

xgb_params <- parameters(
  min_n(),
  tree_depth(),
  learn_rate(),
  loss_reduction()
)

xgb_grid <- grid_max_entropy(
  xgb_params,
  size = 60
)


# define the workflow

xgb_wf <- workflow() %>% 
  add_model(xgb_model) %>% 
  add_formula(CL_cat ~ .)

# tune the model
## hyperparameter tuning

xgb_tuned_60 <- tune_grid(
  object = xgb_wf,
  resamples = xgb_cv_folds,
  grid = xgb_grid,
  metrics = metric_set(sensitivity, accuracy),
  control = control_grid(verbose = T)
)

save(xgb_tuned_60, file = 'data/CL_xgb_tuned_60')

## 70
quantile_data <- data %>% 
  filter(CL > 0) %>% 
  select(CL)

quantile <- (quantile_data$CL %>% 
  quantile(.7))[[1]]

data_cat <- data %>% 
  mutate(CL_cat = case_when(CL < quantile ~ 'low',
                         CL >= quantile ~ 'high') %>% 
           as_factor()) %>% 
  select(-c(TEMP_pland_forest, TEMP_te_forest, TEMP_enn_mn_forest, TEMP_area_mn_forest))

set.seed(123)

# data_sample <- sample_n(data, size = nrow(data) * 0.7)

split_cat <- initial_split(data_cat, prop = 0.7, strata = CL_cat)
train_cat <- training(split_cat) %>% as.data.frame()
test_cat <- testing(split_cat)


library(tidyverse)
library(tidymodels)

# preprocessing recipe
xgb_recipe <-
  recipe(
    CL_cat ~ .,
    data = train_cat) %>% 
  step_rm(CL, Name, Year, Code, Country) %>% 
  prep()

# splitting for cross validation
xgb_cv_folds <- xgb_recipe %>% 
  bake(train_cat) %>% 
  vfold_cv(v = 5)

# xgboost model specification

xgb_model <- boost_tree(
  mode = 'classification',
  trees = 1000,
  min_n = tune(),
  tree_depth = tune(),
  learn_rate = tune(),
  loss_reduction = tune()
  ) %>% 
  set_engine('xgboost', 
             objective ='binary:logistic')

set.seed(123)

# grid specification

xgb_params <- parameters(
  min_n(),
  tree_depth(),
  learn_rate(),
  loss_reduction()
)

xgb_grid <- grid_max_entropy(
  xgb_params,
  size = 60
)


# define the workflow

xgb_wf <- workflow() %>% 
  add_model(xgb_model) %>% 
  add_formula(CL_cat ~ .)

# tune the model
## hyperparameter tuning

xgb_tuned_70 <- tune_grid(
  object = xgb_wf,
  resamples = xgb_cv_folds,
  grid = xgb_grid,
  metrics = metric_set(sensitivity, accuracy),
  control = control_grid(verbose = T)
)

save(xgb_tuned_70, file = 'data/CL_xgb_tuned_70')
```

```{r}
load('data/CL_xgb_tuned_30')
load('data/CL_xgb_tuned_40')
load('data/CL_xgb_tuned_50')
load('data/CL_xgb_tuned_60')
load('data/CL_xgb_tuned_70')

xgb_tuned_30 %>% 
  show_best(metric = 'accuracy')

xgb_tuned_40 %>% 
  show_best(metric = 'accuracy')

xgb_tuned_50 %>% 
  show_best(metric = 'accuracy')

xgb_tuned_60 %>% 
  show_best(metric = 'accuracy')

xgb_tuned_70 %>% 
  show_best(metric = 'accuracy')


xgb_30_best_params <- xgb_tuned_30 %>% 
  select_best(metric = 'sensitivity')

xgb_40_best_params <- xgb_tuned_40 %>% 
  select_best(metric = 'sensitivity')

xgb_50_best_params <- xgb_tuned_50 %>% 
  select_best(metric = 'sensitivity')

xgb_60_best_params <- xgb_tuned_60 %>% 
  select_best(metric = 'sensitivity')

xgb_70_best_params <- xgb_tuned_70 %>% 
  select_best(metric = 'sensitivity')

# create final models based on best parameters

xgb_30_model_final <- xgb_model %>% 
  finalize_model(xgb_30_best_params)

xgb_40_model_final <- xgb_model %>% 
  finalize_model(xgb_40_best_params)

xgb_50_model_final <- xgb_model %>% 
  finalize_model(xgb_50_best_params)

xgb_60_model_final <- xgb_model %>% 
  finalize_model(xgb_60_best_params)

xgb_70_model_final <- xgb_model %>% 
  finalize_model(xgb_70_best_params)
```


```{r}
# evaluate on testing data

## 30
quantile_data <- data %>% 
  filter(CL > 0) %>% 
  select(CL)

quantile <- (quantile_data$CL %>% 
  quantile(.3))[[1]]

data_cat <- data %>%
  mutate(CL_cat = case_when(CL < quantile ~ 'low',
                         CL >= quantile ~ 'high') %>% 
           as_factor()) %>% 
  select(-c(TEMP_pland_forest, TEMP_te_forest, TEMP_enn_mn_forest, TEMP_area_mn_forest))

set.seed(123)

# data_sample <- sample_n(data, size = nrow(data) * 0.7)

split_cat <- initial_split(data_cat, prop = 0.7, strata = CL_cat)
train_cat <- training(split_cat) %>% as.data.frame()
test_cat <- testing(split_cat)

train_processed_30 <- bake(xgb_recipe, train_cat)

train_prediction_30 <- xgb_30_model_final %>% 
  # fit the model on all training data
  fit(
    formula = CL_cat ~ .,
    data = train_processed_30
  ) %>% 
  # predict on the training data
  predict(new_data = train_processed_30) %>% 
  bind_cols(train_cat)

xgb_CL_train_30 <- 
  train_prediction_30 %>% 
  metrics(CL_cat, .pred_class) %>% 
  mutate(.estimate = format(round(.estimate, 2), big.mark = ","))


# Testing

test_processed_30 <- bake(xgb_recipe, new_data = test_cat)
test_prediction_30 <- xgb_30_model_final %>% 
  # fit on the training data
  fit(
    formula = CL_cat ~ .,
    data = train_processed_30
  ) %>% 
  # use the training model fit to predict the test data
  predict(new_data = test_processed_30) %>% 
  bind_cols(test_cat)

xgb_score_30 <- 
  test_prediction_30 %>% 
  metrics(CL_cat, .pred_class) %>% 
  mutate(.estimate = format(round(.estimate, 2), big.mark = ","))

xgb_score_30
confusionMatrix(reference = test_prediction_30$CL_cat,
                data = test_prediction_30$.pred_class)
```

```{r}
xgb_func <- function(quantile, seed) {
  set.seed(seed)
  
  quantile_data <- data %>% 
    filter(CL > 0) %>% 
    select(CL)

  quantile <- (quantile_data$CL %>% 
    quantile(quantile))[[1]]

  data_cat <- data %>%
    mutate(CL_cat = case_when(CL < quantile ~ 'low',
                           CL >= quantile ~ 'high') %>% 
             as_factor()) %>% 
    select(-c(TEMP_pland_forest, 
              TEMP_te_forest, 
              TEMP_enn_mn_forest,
              TEMP_area_mn_forest))
  
  # data partitioning
  split_cat <- initial_split(data_cat, prop = 0.7, strata = CL_cat)
  train_cat <- training(split_cat) %>% as.data.frame()
  test_cat <- testing(split_cat)
  
  # preprocessing recipe
  xgb_recipe <-
    recipe(
      CL_cat ~ .,
      data = train_cat) %>% 
    step_rm(CL, Name, Year, Code, Country) %>% 
    prep()
  
  # splitting for cross validation
  xgb_cv_folds <- xgb_recipe %>% 
    bake(train_cat) %>% 
    vfold_cv(v = 5)
  
  # xgboost model specification
  
  xgb_model <- boost_tree(
    mode = 'classification',
    trees = 1000,
    min_n = tune(),
    tree_depth = tune(),
    learn_rate = tune(),
    loss_reduction = tune()
    ) %>% 
    set_engine('xgboost', 
               objective ='binary:logistic')
  
  set.seed(123)
  
  # grid specification
  
  xgb_params <- parameters(
    min_n(),
    tree_depth(),
    learn_rate(),
    loss_reduction()
  )
  
  xgb_grid <- grid_max_entropy(
    xgb_params,
    size = 20
  )
  
  
  # define the workflow
  
  xgb_wf <- workflow() %>% 
    add_model(xgb_model) %>% 
    add_formula(CL_cat ~ .)
  
  # tune the model
  ## hyperparameter tuning
  
  xgb_tuned <- tune_grid(
    object = xgb_wf,
    resamples = xgb_cv_folds,
    grid = xgb_grid,
    metrics = metric_set(roc_auc, accuracy),
    control = control_grid(verbose = T)
  )
  
  xgb_best_params <- xgb_tuned %>% 
    select_best(metric = 'roc_auc')
  
  model_final <- xgb_model %>% 
    finalize_model(xgb_best_params)
  
  train_processed <- bake(xgb_recipe, train_cat)
  
  train_prediction <- model_final %>% 
    # fit the model on all training data
    fit(
      formula = CL_cat ~ .,
      data = train_processed
    ) %>% 
    # predict on the training data
    predict(new_data = train_processed) %>% 
    bind_cols(train_cat)
  
  xgb_CL_train <- 
    train_prediction %>% 
    metrics(CL_cat, .pred_class) %>% 
    mutate(.estimate = format(round(.estimate, 2), big.mark = ","))
  
  
  # Testing
  
  test_processed <- bake(xgb_recipe, new_data = test_cat)
  test_prediction <- model_final %>% 
    # fit on the training data
    fit(
      formula = CL_cat ~ .,
      data = train_processed
    ) %>% 
    # use the training model fit to predict the test data
    predict(new_data = test_processed) %>% 
    bind_cols(test_cat)
  
  xgb_score <- 
    test_prediction %>% 
    metrics(CL_cat, .pred_class) %>% 
    mutate(.estimate = format(round(.estimate, 2), big.mark = ","))
  
  xgb_list <- list(data = data_cat, 
                   recipe = xgb_recipe,
                   folds = xgb_cv_folds,
                   tuned = xgb_tuned,
                   final_model = model_final,
                   train_score = xgb_CL_train,
                   test_prediction = test_prediction,
                   test_score = xgb_score)
  
  return(xgb_list)
}
```


```{r}
# 30
xgb_eval_30_test <- xgb_func(quantile = 0.3,
                             seed = 123) # sens included

save(xgb_eval_30_test, file = 'data/xgb_eval_30_test')

confusionMatrix(xgb_eval_30_test$test_prediction$.pred_class,
                xgb_eval_30_test$test_prediction$CL_cat)

# 40
xgb_eval_40_test <- xgb_func(quantile = 0.4,
                             seed = 123) # sens not included

save(xgb_eval_40_test, file = 'data/xgb_eval_40_test')

confusionMatrix(xgb_eval_40_test$test_prediction$.pred_class,
                xgb_eval_40_test$test_prediction$CL_cat)


xgb_eval_40 <- xgb_eval(model_final = xgb_40_model_final,
                        quantile = 0.4,
                        seed = 123)

xgb_eval_50 <- xgb_eval(xgb_50_model_final,
                        0.5,
                        123)

xgb_eval_60 <- xgb_eval(xgb_60_model_final,
                        0.6,
                        123)

xgb_eval_70 <- xgb_eval(xgb_70_model_final,
                        0.7,
                        123)
```

