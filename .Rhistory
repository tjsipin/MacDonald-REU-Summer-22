library(tidymodels)
# preprocessing recipe
xgb_recipe <-
recipe(
CL_cat ~ .,
data = train_cat) %>%
step_rm(CL, Name, Year, Code, Country) %>%
prep()
# splitting for cross validation
xgb_cv_folds <- xgb_recipe %>%
bake(train_cat) %>%
vfold_cv(v = 5)
# xgboost model specification
xgb_model <- boost_tree(
mode = 'classification',
trees = 1000,
min_n = tune(),
tree_depth = tune(),
learn_rate = tune(),
loss_reduction = tune()
) %>%
set_engine('xgboost',
objective ='reg:squarederror')
set.seed(123)
# grid specification
xgb_params <- parameters(
min_n(),
tree_depth(),
learn_rate(),
loss_reduction()
)
xgb_grid <- grid_max_entropy(
xgb_params,
size = 60
)
knitr::kable(head(xgb_grid))
# define the workflow
xgb_wf <- workflow() %>%
add_model(xgb_model) %>%
add_formula(CL_cat ~ .)
# tune the model
## hyperparameter tuning
xgb_tuned_30 <- tune_grid(
object = xgb_wf,
resamples = xgb_cv_folds,
grid = xgb_grid,
metrics = metric_set(sensitivity, accuracy),
control = control_grid(verbose = T)
)
xgb_tuned_30 %>%
show_best(metric = 'sensitivity') %>%
kable()
print(hi)
save(xgb_tuned_30, file = 'data/CL_xgb_tuned_30')
knitr::opts_chunk$set(echo = TRUE)
library(randomForest)
library(ggplot2)
library(finalfit)
library(dplyr)
library(naniar)
library(readr)
library(tidyverse)
library(caret)
library(readr)
library(dplyr)
library(mlbench)
library(rsample)
library(tidyverse)
library(tidymodels)
library(recipes)
xgb_tuned_30 %>%
show_best(metric = 'sensitivity') %>%
kable()
library(knitr)
xgb_tuned_30 %>%
show_best(metric = 'sensitivity') %>%
kable()
## 40
quantile_data <- data %>%
filter(CL > 0) %>%
select(CL)
quantile <- (quantile_data$CL %>%
quantile(.4))[[1]]
data_cat <- data %>%
mutate(CL_cat = case_when(CL < quantile ~ 'low',
CL >= quantile ~ 'high') %>%
as_factor()) %>%
select(-c(TEMP_pland_forest, TEMP_te_forest, TEMP_enn_mn_forest, TEMP_area_mn_forest))
set.seed(123)
# data_sample <- sample_n(data, size = nrow(data) * 0.7)
split_cat <- initial_split(data_cat, prop = 0.7, strata = CL_cat)
train_cat <- training(split_cat) %>% as.data.frame()
test_cat <- testing(split_cat)
library(tidyverse)
library(tidymodels)
# preprocessing recipe
xgb_recipe <-
recipe(
CL_cat ~ .,
data = train_cat) %>%
step_rm(CL, Name, Year, Code, Country) %>%
prep()
# splitting for cross validation
xgb_cv_folds <- xgb_recipe %>%
bake(train_cat) %>%
vfold_cv(v = 5)
# xgboost model specification
xgb_model <- boost_tree(
mode = 'classification',
trees = 1000,
min_n = tune(),
tree_depth = tune(),
learn_rate = tune(),
loss_reduction = tune()
) %>%
set_engine('xgboost',
objective ='reg:squarederror')
set.seed(123)
# grid specification
xgb_params <- parameters(
min_n(),
tree_depth(),
learn_rate(),
loss_reduction()
)
xgb_grid <- grid_max_entropy(
xgb_params,
size = 60
)
knitr::kable(head(xgb_grid))
# define the workflow
xgb_wf <- workflow() %>%
add_model(xgb_model) %>%
add_formula(CL_cat ~ .)
# tune the model
## hyperparameter tuning
xgb_tuned_40 <- tune_grid(
object = xgb_wf,
resamples = xgb_cv_folds,
grid = xgb_grid,
metrics = metric_set(sensitivity, accuracy),
control = control_grid(verbose = T)
)
?set_engine
## 30
quantile_data <- data %>%
filter(CL > 0) %>%
select(CL)
quantile <- (quantile_data$CL %>%
quantile(.3))[[1]]
data_cat <- data %>%
mutate(CL_cat = case_when(CL < quantile ~ 'low',
CL >= quantile ~ 'high') %>%
as_factor()) %>%
select(-c(TEMP_pland_forest, TEMP_te_forest, TEMP_enn_mn_forest, TEMP_area_mn_forest))
set.seed(123)
# data_sample <- sample_n(data, size = nrow(data) * 0.7)
split_cat <- initial_split(data_cat, prop = 0.7, strata = CL_cat)
train_cat <- training(split_cat) %>% as.data.frame()
test_cat <- testing(split_cat)
library(tidyverse)
library(tidymodels)
# preprocessing recipe
xgb_recipe <-
recipe(
CL_cat ~ .,
data = train_cat) %>%
step_rm(CL, Name, Year, Code, Country) %>%
prep()
# splitting for cross validation
xgb_cv_folds <- xgb_recipe %>%
bake(train_cat) %>%
vfold_cv(v = 5)
# xgboost model specification
xgb_model <- boost_tree(
mode = 'classification',
trees = 1000,
min_n = tune(),
tree_depth = tune(),
learn_rate = tune(),
loss_reduction = tune()
) %>%
set_engine('xgboost',
objective ='binary:logistic')
set.seed(123)
# grid specification
xgb_params <- parameters(
min_n(),
tree_depth(),
learn_rate(),
loss_reduction()
)
xgb_grid <- grid_max_entropy(
xgb_params,
size = 60
)
knitr::kable(head(xgb_grid))
# define the workflow
xgb_wf <- workflow() %>%
add_model(xgb_model) %>%
add_formula(CL_cat ~ .)
# tune the model
## hyperparameter tuning
xgb_tuned_30 <- tune_grid(
object = xgb_wf,
resamples = xgb_cv_folds,
grid = xgb_grid,
metrics = metric_set(sensitivity, accuracy),
control = control_grid(verbose = T)
)
xgb_tuned_30 %>%
show_best(metric = 'sensitivity') %>%
kable()
?show_best
xgb_tuned_30 %>%
show_best(metric = 'rsq')
xgb_tuned_30 %>%
show_best(metric = 'accuracy') %>%
kable()
xgb_tuned_30 %>%
show_best(metric = 'accuracy')
knitr::opts_chunk$set(echo = TRUE)
library(randomForest)
library(ggplot2)
library(finalfit)
library(dplyr)
library(naniar)
library(readr)
library(tidyverse)
library(caret)
library(readr)
library(dplyr)
library(mlbench)
library(rsample)
library(tidyverse)
library(tidymodels)
library(recipes)
save(xgb_tuned_30, file = 'data/CL_xgb_tuned_30')
## 40
quantile_data <- data %>%
filter(CL > 0) %>%
select(CL)
quantile <- (quantile_data$CL %>%
quantile(.4))[[1]]
data_cat <- data %>%
mutate(CL_cat = case_when(CL < quantile ~ 'low',
CL >= quantile ~ 'high') %>%
as_factor()) %>%
select(-c(TEMP_pland_forest, TEMP_te_forest, TEMP_enn_mn_forest, TEMP_area_mn_forest))
set.seed(123)
# data_sample <- sample_n(data, size = nrow(data) * 0.7)
split_cat <- initial_split(data_cat, prop = 0.7, strata = CL_cat)
train_cat <- training(split_cat) %>% as.data.frame()
test_cat <- testing(split_cat)
library(tidyverse)
library(tidymodels)
# preprocessing recipe
xgb_recipe <-
recipe(
CL_cat ~ .,
data = train_cat) %>%
step_rm(CL, Name, Year, Code, Country) %>%
prep()
# splitting for cross validation
xgb_cv_folds <- xgb_recipe %>%
bake(train_cat) %>%
vfold_cv(v = 5)
# xgboost model specification
xgb_model <- boost_tree(
mode = 'classification',
trees = 1000,
min_n = tune(),
tree_depth = tune(),
learn_rate = tune(),
loss_reduction = tune()
) %>%
set_engine('xgboost',
objective ='binary:logistic')
set.seed(123)
# grid specification
xgb_params <- parameters(
min_n(),
tree_depth(),
learn_rate(),
loss_reduction()
)
xgb_grid <- grid_max_entropy(
xgb_params,
size = 60
)
# define the workflow
xgb_wf <- workflow() %>%
add_model(xgb_model) %>%
add_formula(CL_cat ~ .)
# tune the model
## hyperparameter tuning
xgb_tuned_40 <- tune_grid(
object = xgb_wf,
resamples = xgb_cv_folds,
grid = xgb_grid,
metrics = metric_set(sensitivity, accuracy),
control = control_grid(verbose = T)
)
save(xgb_tuned_40, file = 'data/CL_xgb_tuned_40')
## 50
quantile_data <- data %>%
filter(CL > 0) %>%
select(CL)
quantile <- (quantile_data$CL %>%
quantile(.5))[[1]]
data_cat <- data %>%
mutate(CL_cat = case_when(CL < quantile ~ 'low',
CL >= quantile ~ 'high') %>%
as_factor()) %>%
select(-c(TEMP_pland_forest, TEMP_te_forest, TEMP_enn_mn_forest, TEMP_area_mn_forest))
set.seed(123)
# data_sample <- sample_n(data, size = nrow(data) * 0.7)
split_cat <- initial_split(data_cat, prop = 0.7, strata = CL_cat)
train_cat <- training(split_cat) %>% as.data.frame()
test_cat <- testing(split_cat)
library(tidyverse)
library(tidymodels)
# preprocessing recipe
xgb_recipe <-
recipe(
CL_cat ~ .,
data = train_cat) %>%
step_rm(CL, Name, Year, Code, Country) %>%
prep()
# splitting for cross validation
xgb_cv_folds <- xgb_recipe %>%
bake(train_cat) %>%
vfold_cv(v = 5)
# xgboost model specification
xgb_model <- boost_tree(
mode = 'classification',
trees = 1000,
min_n = tune(),
tree_depth = tune(),
learn_rate = tune(),
loss_reduction = tune()
) %>%
set_engine('xgboost',
objective ='binary:logistic')
set.seed(123)
# grid specification
xgb_params <- parameters(
min_n(),
tree_depth(),
learn_rate(),
loss_reduction()
)
xgb_grid <- grid_max_entropy(
xgb_params,
size = 60
)
# define the workflow
xgb_wf <- workflow() %>%
add_model(xgb_model) %>%
add_formula(CL_cat ~ .)
# tune the model
## hyperparameter tuning
xgb_tuned_50 <- tune_grid(
object = xgb_wf,
resamples = xgb_cv_folds,
grid = xgb_grid,
metrics = metric_set(sensitivity, accuracy),
control = control_grid(verbose = T)
)
save(xgb_tuned_50, file = 'data/CL_xgb_tuned_50')
## 60
quantile_data <- data %>%
filter(CL > 0) %>%
select(CL)
quantile <- (quantile_data$CL %>%
quantile(.6))[[1]]
data_cat <- data %>%
mutate(CL_cat = case_when(CL < quantile ~ 'low',
CL >= quantile ~ 'high') %>%
as_factor()) %>%
select(-c(TEMP_pland_forest, TEMP_te_forest, TEMP_enn_mn_forest, TEMP_area_mn_forest))
set.seed(123)
# data_sample <- sample_n(data, size = nrow(data) * 0.7)
split_cat <- initial_split(data_cat, prop = 0.7, strata = CL_cat)
train_cat <- training(split_cat) %>% as.data.frame()
test_cat <- testing(split_cat)
library(tidyverse)
library(tidymodels)
# preprocessing recipe
xgb_recipe <-
recipe(
CL_cat ~ .,
data = train_cat) %>%
step_rm(CL, Name, Year, Code, Country) %>%
prep()
# splitting for cross validation
xgb_cv_folds <- xgb_recipe %>%
bake(train_cat) %>%
vfold_cv(v = 5)
# xgboost model specification
xgb_model <- boost_tree(
mode = 'classification',
trees = 1000,
min_n = tune(),
tree_depth = tune(),
learn_rate = tune(),
loss_reduction = tune()
) %>%
set_engine('xgboost',
objective ='binary:logistic')
set.seed(123)
# grid specification
xgb_params <- parameters(
min_n(),
tree_depth(),
learn_rate(),
loss_reduction()
)
xgb_grid <- grid_max_entropy(
xgb_params,
size = 60
)
# define the workflow
xgb_wf <- workflow() %>%
add_model(xgb_model) %>%
add_formula(CL_cat ~ .)
# tune the model
## hyperparameter tuning
xgb_tuned_60 <- tune_grid(
object = xgb_wf,
resamples = xgb_cv_folds,
grid = xgb_grid,
metrics = metric_set(sensitivity, accuracy),
control = control_grid(verbose = T)
)
save(xgb_tuned_60, file = 'data/CL_xgb_tuned_60')
## 70
quantile_data <- data %>%
filter(CL > 0) %>%
select(CL)
quantile <- (quantile_data$CL %>%
quantile(.7))[[1]]
data_cat <- data %>%
mutate(CL_cat = case_when(CL < quantile ~ 'low',
CL >= quantile ~ 'high') %>%
as_factor()) %>%
select(-c(TEMP_pland_forest, TEMP_te_forest, TEMP_enn_mn_forest, TEMP_area_mn_forest))
set.seed(123)
# data_sample <- sample_n(data, size = nrow(data) * 0.7)
split_cat <- initial_split(data_cat, prop = 0.7, strata = CL_cat)
train_cat <- training(split_cat) %>% as.data.frame()
test_cat <- testing(split_cat)
library(tidyverse)
library(tidymodels)
# preprocessing recipe
xgb_recipe <-
recipe(
CL_cat ~ .,
data = train_cat) %>%
step_rm(CL, Name, Year, Code, Country) %>%
prep()
# splitting for cross validation
xgb_cv_folds <- xgb_recipe %>%
bake(train_cat) %>%
vfold_cv(v = 5)
# xgboost model specification
xgb_model <- boost_tree(
mode = 'classification',
trees = 1000,
min_n = tune(),
tree_depth = tune(),
learn_rate = tune(),
loss_reduction = tune()
) %>%
set_engine('xgboost',
objective ='binary:logistic')
set.seed(123)
# grid specification
xgb_params <- parameters(
min_n(),
tree_depth(),
learn_rate(),
loss_reduction()
)
xgb_grid <- grid_max_entropy(
xgb_params,
size = 60
)
# define the workflow
xgb_wf <- workflow() %>%
add_model(xgb_model) %>%
add_formula(CL_cat ~ .)
# tune the model
## hyperparameter tuning
xgb_tuned_70 <- tune_grid(
object = xgb_wf,
resamples = xgb_cv_folds,
grid = xgb_grid,
metrics = metric_set(sensitivity, accuracy),
control = control_grid(verbose = T)
)
save(xgb_tuned_70, file = 'data/CL_xgb_tuned_70')
xgb_tuned_30 %>%
show_best(metric = 'accuracy')
xgb_tuned_30 %>%
show_best(metric = 'sensitivity')
xgb_tuned_30 %>%
show_best(metric = 'specificity')
xgb_tuned_30 %>%
show_best(metric = 'sensitivity')
xgb_tuned_30 %>%
show_best(metric = 'accuracy')
